{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root node: Date\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"C:\\Users\\Dell\\Downloads\\DATA (3)\\DATA\")  # Replace \"your_dataset.csv\" with the actual filename\n",
    "\n",
    "# Define function for information gain calculation\n",
    "def calculate_information_gain(data, feature_name, target_name):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for a given feature.\n",
    "    \"\"\"\n",
    "    # Calculate entropy of the parent node\n",
    "    total_entropy = calculate_entropy(data[target_name])\n",
    "\n",
    "    # Calculate weighted average of entropy for child nodes\n",
    "    weighted_entropy = 0\n",
    "    for value in data[feature_name].unique():\n",
    "        subset = data[data[feature_name] == value]\n",
    "        weighted_entropy += (len(subset) / len(data)) * calculate_entropy(subset[target_name])\n",
    "\n",
    "    # Calculate information gain\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "# Define function to calculate entropy\n",
    "def calculate_entropy(target):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a target variable.\n",
    "    \"\"\"\n",
    "    entropy = 0\n",
    "    classes = target.unique()\n",
    "    total_count = len(target)\n",
    "    for c in classes:\n",
    "        count = len(target[target == c])\n",
    "        p = count / total_count\n",
    "        entropy -= p * np.log2(p)\n",
    "    return entropy\n",
    "\n",
    "# Define function for binning continuous features\n",
    "def bin_continuous_feature(data, feature_name, num_bins, binning_type=\"equal_width\"):\n",
    "    \"\"\"\n",
    "    Convert a continuous feature to categorical by binning.\n",
    "    \"\"\"\n",
    "    if binning_type == \"equal_width\":\n",
    "        bins = np.linspace(data[feature_name].min(), data[feature_name].max(), num_bins + 1)\n",
    "    elif binning_type == \"frequency\":\n",
    "        bins = np.percentile(data[feature_name], np.arange(0, 100, 100 / num_bins))\n",
    "        bins[-1] = data[feature_name].max()  # Ensure the last bin includes max value\n",
    "    else:\n",
    "        raise ValueError(\"Invalid binning type. Use 'equal_width' or 'frequency'.\")\n",
    "\n",
    "    binned_feature = pd.cut(data[feature_name], bins, labels=range(num_bins))\n",
    "    return binned_feature\n",
    "\n",
    "# Function to find the best split feature for the root node\n",
    "def find_root_node(data, target_name, binning_type=\"equal_width\", num_bins=5):\n",
    "    \"\"\"\n",
    "    Find the best split feature for the root node using information gain.\n",
    "    \"\"\"\n",
    "    features = data.columns.tolist()\n",
    "    features.remove(target_name)\n",
    "\n",
    "    best_feature = None\n",
    "    max_information_gain = -float('inf')\n",
    "\n",
    "    for feature in features:\n",
    "        if data[feature].dtype == 'object':\n",
    "            information_gain = calculate_information_gain(data, feature, target_name)\n",
    "        else:\n",
    "            # Convert continuous feature to categorical\n",
    "            binned_feature = bin_continuous_feature(data, feature, num_bins, binning_type)\n",
    "            data_binned = data.copy()\n",
    "            data_binned[feature] = binned_feature\n",
    "            information_gain = calculate_information_gain(data_binned, feature, target_name)\n",
    "\n",
    "        if information_gain > max_information_gain:\n",
    "            max_information_gain = information_gain\n",
    "            best_feature = feature\n",
    "\n",
    "    return best_feature\n",
    "\n",
    "# Test the function with the provided dataset\n",
    "root_node = find_root_node(data, \"Category\")\n",
    "print(\"Root node:\", root_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root node: Date\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"C:\\Users\\Dell\\Downloads\\DATA (3)\\DATA\")  # Replace \"your_dataset.csv\" with the actual filename\n",
    "\n",
    "# Define function for information gain calculation\n",
    "def calculate_information_gain(data, feature_name, target_name):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for a given feature.\n",
    "    \"\"\"\n",
    "    # Calculate entropy of the parent node\n",
    "    total_entropy = calculate_entropy(data[target_name])\n",
    "\n",
    "    # Calculate weighted average of entropy for child nodes\n",
    "    weighted_entropy = 0\n",
    "    for value in data[feature_name].unique():\n",
    "        subset = data[data[feature_name] == value]\n",
    "        weighted_entropy += (len(subset) / len(data)) * calculate_entropy(subset[target_name])\n",
    "\n",
    "    # Calculate information gain\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "# Define function to calculate entropy\n",
    "def calculate_entropy(target):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a target variable.\n",
    "    \"\"\"\n",
    "    entropy = 0\n",
    "    classes = target.unique()\n",
    "    total_count = len(target)\n",
    "    for c in classes:\n",
    "        count = len(target[target == c])\n",
    "        p = count / total_count\n",
    "        entropy -= p * np.log2(p)\n",
    "    return entropy\n",
    "\n",
    "# Define function for binning continuous features\n",
    "def bin_continuous_feature(data, feature_name, num_bins, binning_type=\"equal_width\"):\n",
    "    \"\"\"\n",
    "    Convert a continuous feature to categorical by binning.\n",
    "    \"\"\"\n",
    "    if binning_type == \"equal_width\":\n",
    "        bins = np.linspace(data[feature_name].min(), data[feature_name].max(), num_bins + 1)\n",
    "    elif binning_type == \"frequency\":\n",
    "        bins = np.percentile(data[feature_name], np.arange(0, 100, 100 / num_bins))\n",
    "        bins[-1] = data[feature_name].max()  # Ensure the last bin includes max value\n",
    "    else:\n",
    "        raise ValueError(\"Invalid binning type. Use 'equal_width' or 'frequency'.\")\n",
    "\n",
    "    binned_feature = pd.cut(data[feature_name], bins, labels=range(num_bins))\n",
    "    return binned_feature\n",
    "\n",
    "# Function to find the best split feature for the root node\n",
    "def find_root_node(data, target_name, binning_type=\"equal_width\", num_bins=5):\n",
    "    \"\"\"\n",
    "    Find the best split feature for the root node using information gain.\n",
    "    \"\"\"\n",
    "    features = data.columns.tolist()\n",
    "    features.remove(target_name)\n",
    "\n",
    "    best_feature = None\n",
    "    max_information_gain = -float('inf')\n",
    "\n",
    "    for feature in features:\n",
    "        if data[feature].dtype == 'object':\n",
    "            information_gain = calculate_information_gain(data, feature, target_name)\n",
    "        else:\n",
    "            # Convert continuous feature to categorical\n",
    "            binned_feature = bin_continuous_feature(data, feature, num_bins, binning_type)\n",
    "            data_binned = data.copy()\n",
    "            data_binned[feature] = binned_feature\n",
    "            information_gain = calculate_information_gain(data_binned, feature, target_name)\n",
    "\n",
    "        if information_gain > max_information_gain:\n",
    "            max_information_gain = information_gain\n",
    "            best_feature = feature\n",
    "\n",
    "    return best_feature\n",
    "\n",
    "# Test the function with the provided dataset\n",
    "root_node = find_root_node(data, \"Chg%\")\n",
    "print(\"Root node:\", root_node)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
